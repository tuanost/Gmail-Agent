# Gmail Agent - Template Cấu Hình Môi Trường
# Sao chép file này thành .env và điền thông tin cần thiết

# API Keys cho các mô hình AI
OPENAI_API_KEY=your_openai_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# Cấu hình mô hình AI mặc định
DEFAULT_AI_PROVIDER=gemini  # Chọn "gemini" hoặc "openai" hoặc "ollama"
#DEFAULT_AI_PROVIDER=openai
#DEFAULT_AI_PROVIDER=ollama

# Cấu hình tên mô hình
DEFAULT_OPENAI_MODEL=gpt-3.5-turbo-0125
DEFAULT_GEMINI_MODEL=gemini-1.5-flash
DEFAULT_OLLAMA_MODEL=llama3

# Default prompt cho phân tích email
DEFAULT_EMAIL_PROMPT="Bạn hãy đọc email và tóm tắt lại những ý chính, highlight các keywords cần thiết để tôi có nắm thông tin nhanh hơn. Hãy chú ý các thông tin về người gửi và người nhận trong phần METADATA."

# Prompt cho phân tích lỗi pipeline
PIPELINE_ERROR_PROMPT="Là một chuyên gia phân tích lỗi CI/CD pipeline, hãy phân tích chi tiết log lỗi sau và đưa ra gợi ý cụ thể để khắc phục:

THÔNG TIN DỰ ÁN:
- Tên dự án: {project_name}
- Commit ID: {commit_id}
- Môi trường: {environment}
- Phân loại lỗi ban đầu: {error_type}

CÁC DÒNG LỖI ĐÃ PHÁT HIỆN:
{error_lines}

LOG ĐẦY ĐỦ:
```
{logs}
```

Hãy cung cấp phân tích chi tiết theo định dạng sau:
1. PHÂN TÍCH LỖI:
   - Nguyên nhân chính: [nguyên nhân gốc rễ của vấn đề]
   - Phân loại lỗi: [xác định chính xác loại lỗi]
   - Files/components bị ảnh hưởng: [liệt kê các file/component liên quan đến lỗi]

2. GIẢI PHÁP ĐỀ XUẤT:
   - Cách khắc phục nhanh: [giải pháp ngắn gọn để fix lỗi]
   - Giải pháp lâu dài: [đề xuất cải tiến để tránh lỗi trong tương lai]
   - Code mẫu (nếu áp dụng): [cung cấp code mẫu để sửa lỗi]

3. KHUYẾN NGHỊ BỔ SUNG:
   - Best practices: [các biện pháp tốt nhất liên quan]
   - Kiểm tra thêm: [các phần khác của hệ thống nên kiểm tra]

Hãy giải thích rõ ràng để cả người mới và người có kinh nghiệm đều có thể hiểu và áp dụng được giải pháp."

# Cấu hình GitLab (sử dụng cho phân tích pipeline GitLab)
GITLAB_API_URL=https://gitlab.com/api/v4
GITLAB_API_TOKEN=your_gitlab_token_here

# Cấu hình Ollama (chỉ áp dụng khi sử dụng Ollama)
OLLAMA_BASE_URL=http://localhost:11434  # URL cơ sở cho API của Ollama

# Cấu hình proxy cho môi trường công ty (nếu cần)
PROXY_ENABLED=False
HTTP_PROXY=
HTTPS_PROXY=

# Cấu hình logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
